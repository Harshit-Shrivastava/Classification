init_wt = init_wt*exp(alphat_ht2*correct_ht2)
if(eht1 < eht2){
ct[j] = 1
}
else{
ct[j] = 2
}
}
plot(ct, xlab="Points in X", ylab='Classifier used',xlim=c(0,1000),ylim=c(0,2),type = 'l')
#Adapted from Professor's class demostration
iter_no = rep(0,100)
for(a in 1:100){
m=1  # starting point for for loop
n=50 # number of samples in each cluster
K=4  # number of clusters
d=2  # number of dimensions
#initializing an empty matrix
X = matrix(NA,n*K,d)
init_cluster = matrix(NA,n*K,1)
#populating the data
for(k in 1:K){
Tk = matrix(rnorm(4),2,2)
bk =matrix(rnorm(2,mean=0,sd=10),2,1)
for(i in m:n) {
z = matrix(rnorm(2),2,1)
X[i,] = Tk %*% z + bk
init_cluster[i] = k
}
m=n+1
n = n+50
}
#reinitializing the index to 50
n=50
#plotting initial data
plot(X,col=init_cluster, main="Data from part a")
proto = matrix(rnorm(K*d),K,d) # proto[k,] is the center (prototype) of kth cluster
dist = matrix(NA,K,n*K) # dist[k,i] is the distance from ith point to kth cluster center
cluster = rep(0,n)
diff = rep(0,K)
for(iter in 1:15){
for(i in 1:200){
for(k in 1:K){
dist[k,i] = (X[i,1]-proto[k,1])^2 + (X[i,2]-proto[k,2])^2
}
cluster[i] = which.min(dist[,i])
}
plot(X,col=cluster)
points(proto,pch='x',col=1:K,cex=3)
for(k in 1:K) {
if (sum(cluster == k) == 0) next;
if (sum(cluster == k) == 1) next;	# don't reestimate center if only have 0-1 examples
if(all(proto[k,] == colMeans(X[cluster==k,]))){   # reestimate kth cluster center
diff[k] =FALSE
}
else{
diff[k] =TRUE
}
proto[k,] = colMeans(X[cluster==k,])
}
if(sum(diff == FALSE) ==4){
iter_no[a] = iter-1
break
}
}
}
#Plot the number of steps to convergence for 100 datasets
plot(iter_no,xlab="Dataset Number", ylab='Number of steps to convergence',xlim=c(0,100),ylim=c(0,20),type = 'l')
#Adapted from Professor's class demostration
iter_no = rep(0,100)
for(a in 1:100){
m=1  # starting point for for loop
n=50 # number of samples in each cluster
K=4  # number of clusters
d=2  # number of dimensions
#initializing an empty matrix
X = matrix(NA,n*K,d)
init_cluster = matrix(NA,n*K,1)
#populating the data
for(k in 1:K){
Tk = matrix(rnorm(4),2,2)
bk =matrix(rnorm(2,mean=0,sd=10),2,1)
for(i in m:n) {
z = matrix(rnorm(2),2,1)
X[i,] = Tk %*% z + bk
init_cluster[i] = k
}
m=n+1
n = n+50
}
#reinitializing the index to 50
n=50
#plotting initial data
plot(X,col=init_cluster, main="Data from part a")
proto = matrix(rnorm(K*d),K,d) # proto[k,] is the center (prototype) of kth cluster
dist = matrix(NA,K,n*K) # dist[k,i] is the distance from ith point to kth cluster center
cluster = rep(0,n)
diff = rep(0,K)
for(iter in 1:15){
for(i in 1:200){
for(k in 1:K){
dist[k,i] = (X[i,1]-proto[k,1])^2 + (X[i,2]-proto[k,2])^2
}
cluster[i] = which.min(dist[,i])
}
plot(X,col=cluster)
points(proto,pch='x',col=1:K,cex=3)
for(k in 1:K) {
if (sum(cluster == k) == 0) next;
if (sum(cluster == k) == 1) next;	# don't reestimate center if only have 0-1 examples
if(all(proto[k,] == colMeans(X[cluster==k,]))){   # reestimate kth cluster center
diff[k] =FALSE
}
else{
diff[k] =TRUE
}
proto[k,] = colMeans(X[cluster==k,])
}
if(sum(diff == FALSE) ==4){
iter_no[a] = iter-1
break
}
}
}
N=1000
x = matrix(rnorm(N),N,1)
y = matrix(NA,N,1)
error = 0
for (n in 1:N) {
trueclass = sample(x,1);
if (abs(trueclass) < 1) { p = c(0.1,0.9) }
else { p = c(0.9,0.1) }
y[n] = sample(c(-1,1),1,prob=p);
}
init_wt = rep(1/N,N)
init_ct = sample(x,1)
ct = 0
ht1 <- function(x,ct){
if (x>ct){
y = -1
}
else{
y = 1
}
y
}
ht2 <- function(x,ct){
if (x>ct){
y = 1
}
else{
y = -1
}
y
}
y_est_ht1 = rep(0,N)
y_est_ht2 =rep(0,N)
error_ht1 = 0
error_ht2 = 0
correct_ht1 = rep(0,N)
correct_ht2 = rep(0,N)
ct = matrix(NA, 1000, 1)
eht1 = 0
eht2 = 0
for(j in 1:n){
for(i in 1:n){
y_est_ht1[i] = ht1(x[i],init_ct)
y_est_ht2[i] = ht2(x[i],init_ct)
if(y_est_ht1[i] == y[i]){
correct_ht1[i] =0
}else{
correct_ht1[i] = 1
}
#error on ht1
eht1 = sum(correct_ht1)
if(y_est_ht2[i] == y[i]){
correct_ht2[i] =0
}else{
correct_ht2[i] = 1
}
#error on ht2
eht2 = sum(correct_ht2)
error_ht1 = error_ht1 + sum(init_wt[i]*correct_ht1[i])
error_ht2 = error_ht2 + sum(init_wt[i]*correct_ht2[i])
}
alphat_ht1 = 0
alphat_ht2 = 0
if(error_ht1 > error_ht2){
alphat_ht2 = 0.5* (log(1-error_ht2/1000)/(error_ht2/1000))
}else{
alphat_ht1 = 0.5* (log(1-error_ht1/1000)/(error_ht1/1000))
}
init_wt = init_wt*exp(alphat_ht2*correct_ht2)
#store the lesser of the two as the classifier for this point
if(eht1 < eht2){
ct[j,] = 1
}
else{
ct[j,] = 2
}
}
#plot all the classifiers
plot(ct, xlab="Points in X", ylab='Classifier used',xlim=c(0,1000),ylim=c(0,2),type = 'l')
N=1000
x = matrix(rnorm(N),N,1)
y = matrix(NA,N,1)
error = 0
for (n in 1:N) {
trueclass = sample(x,1);
if (abs(trueclass) < 1) { p = c(0.1,0.9) }
else { p = c(0.9,0.1) }
y[n] = sample(c(-1,1),1,prob=p);
}
init_wt = rep(1/N,N)
init_ct = sample(x,1)
ct = 0
ht1 <- function(x,ct){
if (x>ct){
y = -1
}
else{
y = 1
}
y
}
ht2 <- function(x,ct){
if (x>ct){
y = 1
}
else{
y = -1
}
y
}
y_est_ht1 = rep(0,N)
y_est_ht2 =rep(0,N)
error_ht1 = 0
error_ht2 = 0
correct_ht1 = rep(0,N)
correct_ht2 = rep(0,N)
classifier = matrix(NA, 1000, 1)
eht1 = 0
eht2 = 0
for(j in 1:n){
for(i in 1:n){
y_est_ht1[i] = ht1(x[i],init_ct)
y_est_ht2[i] = ht2(x[i],init_ct)
if(y_est_ht1[i] == y[i]){
correct_ht1[i] =0
}else{
correct_ht1[i] = 1
}
#error on ht1
eht1 = sum(correct_ht1)
if(y_est_ht2[i] == y[i]){
correct_ht2[i] =0
}else{
correct_ht2[i] = 1
}
#error on ht2
eht2 = sum(correct_ht2)
error_ht1 = error_ht1 + sum(init_wt[i]*correct_ht1[i])
error_ht2 = error_ht2 + sum(init_wt[i]*correct_ht2[i])
}
alpha_ht1 = 0
alpha_ht2 = 0
if(error_ht1 > error_ht2){
alpha_ht2 = 0.5* (log(1-error_ht2/1000)/(error_ht2/1000))
}else{
alpha_ht1 = 0.5* (log(1-error_ht1/1000)/(error_ht1/1000))
}
init_wt = init_wt*exp(alpha_ht2*correct_ht2)
#store the lesser of the two as the classifier for this point
if(eht1 < eht2){
classifier[j,] = 1
}
else{
classifier[j,] = 2
}
}
#plot all the classifiers
plot(classifier, xlab="Points in data", ylab='Classifier used',xlim=c(0,1000),ylim=c(0,2),type = 'l')
N=1000
x = matrix(rnorm(N),N,1)
y = matrix(NA,N,1)
for (n in 1:N) {
trueclass = sample(x,1);
if (abs(trueclass) < 1) { p = c(0.1,0.9) }
else { p = c(0.9,0.1) }
y[n] = sample(c(-1,1),1,prob=p);
}
init_wt = rep(1/N,N)
init_ct = sample(x,1)
ct = 0
ht1 <- function(x,ct){
if (x>ct){
y = -1
}
else{
y = 1
}
y
}
ht2 <- function(x,ct){
if (x>ct){
y = 1
}
else{
y = -1
}
y
}
y_est_ht1 = rep(0,N)
y_est_ht2 =rep(0,N)
error_ht1 = 0
error_ht2 = 0
correct_ht1 = rep(0,N)
correct_ht2 = rep(0,N)
classifier = matrix(NA, 1000, 1)
eht1 = 0
eht2 = 0
for(j in 1:n){
for(i in 1:n){
y_est_ht1[i] = ht1(x[i],init_ct)
y_est_ht2[i] = ht2(x[i],init_ct)
if(y_est_ht1[i] == y[i]){
correct_ht1[i] =0
}else{
correct_ht1[i] = 1
}
#error on ht1
eht1 = sum(correct_ht1)
if(y_est_ht2[i] == y[i]){
correct_ht2[i] =0
}else{
correct_ht2[i] = 1
}
#error on ht2
eht2 = sum(correct_ht2)
error_ht1 = error_ht1 + sum(init_wt[i]*correct_ht1[i])
error_ht2 = error_ht2 + sum(init_wt[i]*correct_ht2[i])
}
alpha_ht1 = 0
alpha_ht2 = 0
if(error_ht1 > error_ht2){
alpha_ht2 = 0.5* (log(1-error_ht2/1000)/(error_ht2/1000))
}else{
alpha_ht1 = 0.5* (log(1-error_ht1/1000)/(error_ht1/1000))
}
init_wt = init_wt*exp(alpha_ht2*correct_ht2)
#store the lesser of the two as the classifier for this point
if(eht1 < eht2){
classifier[j,] = 1
}
else{
classifier[j,] = 2
}
}
#plot all the classifiers
plot(classifier, xlab="Points in data", ylab='Classifier used',xlim=c(0,1000),ylim=c(0,2),type = 'l')
N=1000
x = matrix(rnorm(N),N,1)
y = matrix(NA,N,1)
for (n in 1:N) {
trueclass = sample(x,1);
if (abs(trueclass) < 1) { p = c(0.1,0.9) }
else { p = c(0.9,0.1) }
y[n] = sample(c(-1,1),1,prob=p);
}
init_wt = rep(1/N,N)
init_ct = sample(x,1)
ct = 0
ht1 <- function(x,ct){
if (x>ct){
y = -1
}
else{
y = 1
}
y
}
ht2 <- function(x,ct){
if (x>ct){
y = 1
}
else{
y = -1
}
y
}
y_est_ht1 = rep(0,N)
y_est_ht2 =rep(0,N)
error_ht1 = 0
error_ht2 = 0
correct_ht1 = rep(0,N)
correct_ht2 = rep(0,N)
classifier = matrix(NA, 1000, 1)
eht1 = 0
eht2 = 0
for(j in 1:n){
for(i in 1:n){
y_est_ht1[i] = ht1(x[i],init_ct)
y_est_ht2[i] = ht2(x[i],init_ct)
if(y_est_ht1[i] == y[i]){
correct_ht1[i] =0
}else{
correct_ht1[i] = 1
}
#error on ht1
eht1 = sum(correct_ht1)
if(y_est_ht2[i] == y[i]){
correct_ht2[i] =0
}else{
correct_ht2[i] = 1
}
#error on ht2
eht2 = sum(correct_ht2)
error_ht1 = error_ht1 + sum(init_wt[i]*correct_ht1[i])
error_ht2 = error_ht2 + sum(init_wt[i]*correct_ht2[i])
}
alpha_ht1 = 0
alpha_ht2 = 0
if(error_ht1 > error_ht2){
alpha_ht2 = 0.5* (log(1-error_ht2/1000)/(error_ht2/1000))
}else{
alpha_ht1 = 0.5* (log(1-error_ht1/1000)/(error_ht1/1000))
}
init_wt = init_wt*exp(alpha_ht2*correct_ht2)
#store the lesser of the two as the classifier for this point
if(eht1 < eht2){
classifier[j,] = 1
}
else{
classifier[j,] = 2
}
}
#plot all the classifiers
plot(classifier, xlab="Points in data", ylab='Classifier used',xlim=c(0,1000),ylim=c(0,2),type = 'l')
data=read.table("page-blocks", sep = "\t")
setwd("C:/SP17/Data Mining/Project")
data=read.table("page-blocks", sep = "\t")
data=read.table("page-blocks.data", sep = "\t")
data
output <- matrix(unlist(data), ncol = 11, byrow = TRUE)
output <- matrix(unlist(data), ncol = 10, byrow = TRUE)
output <- do.call(rbind,lapply(data,matrix,ncol=11,byrow=TRUE))
output
data=read.table("page-blocks.data", sep = "\t")
data
num.el <- sapply(data, length)
# Generate the matrix
res <- cbind(unlist(data), rep(1:length(a), num.el))
data=read.table("page-blocks.data", sep = "\t")
data
num.el <- sapply(data, length)
# Generate the matrix
res <- cbind(unlist(data), rep(1:length(data), num.el))
res
data=read.table("page-blocks.data", sep = "\t")
data
cbind(rep(seq_along(data), times=sapply(data, length)), unlist(data))
data=read.table("page-blocks.data", sep = "\t")
data
output <-cbind(rep(seq_along(data), times=sapply(data, length)), unlist(data))
output
install.packages("rio")
library("rio")
y <- import("page-blocks.data")
y
y <- import("page-blocks.data")
load("page-blocks.data")
ls() #returns a list of all the objects you just loaded (and anything else in your environment)
write.csv(file="yourCSV.csv")
f <-file("http://archive.ics.uci.edu/ml/machine-learning-databases/acute/diagnosis.data", open="r" ,encoding="UTF-16LE")
data <- read.table(f, dec=",", header=F)
data
f <-file("https://archive.ics.uci.edu/ml/machine-learning-databases/page-blocks/page-blocks.data.Z", open="r" ,encoding="UTF-16LE")
data <- read.table(f, dec=",", header=F)
data
f <-file("page-blocks.data", open="r" ,encoding="UTF-16LE")
data <- read.table(f, dec=",", header=F)
data
f <-file("page-blocks.data", open="r" ,encoding="UTF-16LE")
data <- read.table(f, dec=",", header=F)
data
f <-file("page-blocks.data", open="r" ,encoding="UTF-16LE")
data <- read.table(f, dec=",", header=F)
data
f <-file("page-blocks.data", open="r" ,encoding="UTF-16")
data <- read.table(f, dec=",", header=F)
data -< read.arrf("https://archive.ics.uci.edu/ml/machine-learning-databases/00327/Training%20Dataset.arff")
data <- read.arrf("https://archive.ics.uci.edu/ml/machine-learning-databases/00327/Training%20Dataset.arff")
install.packages("foreign")
install.packages("foreign")
data <- read.arrf("https://archive.ics.uci.edu/ml/machine-learning-databases/00327/Training%20Dataset.arff")
library("foreign")
data <- read.arrf("https://archive.ics.uci.edu/ml/machine-learning-databases/00327/Training%20Dataset.arff")
library("foreign", lib.loc="C:/Program Files/R/R-3.3.2/library")
library("foreign")
data <- read.arrf("https://archive.ics.uci.edu/ml/machine-learning-databases/00327/Training%20Dataset.arff")
library("RWeka")
data <- read.arrf("https://archive.ics.uci.edu/ml/machine-learning-databases/00327/Training%20Dataset.arff")
install.packages("RWeka")
library("RWeka")
data <- read.arrf("https://archive.ics.uci.edu/ml/machine-learning-databases/00327/Training%20Dataset.arff")
data = read.table("page-blocks.data", sep="")
data
data = read.table("page-blocks.data", sep="")
data = as.matrix(data)
data = as.matrix(data)
data
scatterplotMatrix(data)
plot(data)
