library('rpart')
library('rpart.plot')
dataset = read.table("../Data/page-blocks.data", sep = "")
dataset = as.matrix(dataset)
##experiments start here
bound <- floor((nrow(dataset)/4)*3)         #define % of training and test set
dataset <- dataset[sample(nrow(dataset)), ]           #sample rows
trainset <- dataset[1:bound, ]              #get training set
testset <- dataset[(bound+1):nrow(dataset), ]    #get test set
#creating the model
model <- rpart(trainset[,11] ~ trainset[,1:10], data = as.data.frame(trainset), minbucket=5)
#plotting the tree, along with attributes and split factors
plot(model)
text(model)
#printing and plotting relative error vs cp and size of tree for pruning
printcp(model)
plotcp(mode)
#testing the model
pred <- predict(model, data = as.data.frame(testset))
#calculating accuracy
accuracy = 1 - mean(pred == testset[ , testset[,11]])
##experiments end here
#fit =  rpart(data[,11] ~ data[,1:10], data=as.data.frame(data))
#rpart.plot(fit)
#class.pred <- table(predict(fit, type="class"), data[,11]) #confusion matrix
#cerror1 = 1-sum(diag(class.pred))/sum(class.pred) #classification error
#cerror1
setwd("C:/SP17/Data Mining/Project/Code")
library('rpart')
library('rpart.plot')
dataset = read.table("../Data/page-blocks.data", sep = "")
dataset = as.matrix(dataset)
##experiments start here
bound <- floor((nrow(dataset)/4)*3)         #define % of training and test set
dataset <- dataset[sample(nrow(dataset)), ]           #sample rows
trainset <- dataset[1:bound, ]              #get training set
testset <- dataset[(bound+1):nrow(dataset), ]    #get test set
#creating the model
model <- rpart(trainset[,11] ~ trainset[,1:10], data = as.data.frame(trainset), minbucket=5)
#plotting the tree, along with attributes and split factors
plot(model)
text(model)
#printing and plotting relative error vs cp and size of tree for pruning
printcp(model)
plotcp(mode)
#testing the model
pred <- predict(model, data = as.data.frame(testset))
#calculating accuracy
accuracy = 1 - mean(pred == testset[ , testset[,11]])
##experiments end here
#fit =  rpart(data[,11] ~ data[,1:10], data=as.data.frame(data))
#rpart.plot(fit)
#class.pred <- table(predict(fit, type="class"), data[,11]) #confusion matrix
#cerror1 = 1-sum(diag(class.pred))/sum(class.pred) #classification error
#cerror1
rpart.plot(model)
library('rpart.plot')
install.packages("rpart.plot")
rpart.plot(model)
library('rpart.plot')
library('rpart')
library('rpart.plot')
dataset = read.table("../Data/page-blocks.data", sep = "")
dataset = as.matrix(dataset)
##experiments start here
bound <- floor((nrow(dataset)/4)*3)         #define % of training and test set
dataset <- dataset[sample(nrow(dataset)), ]           #sample rows
trainset <- dataset[1:bound, ]              #get training set
testset <- dataset[(bound+1):nrow(dataset), ]    #get test set
#creating the model
model <- rpart(trainset[,11] ~ trainset[,1:10], data = as.data.frame(trainset), minbucket=5)
#plotting the tree, along with attributes and split factors
rpart.plot(model)
text(model)
#printing and plotting relative error vs cp and size of tree for pruning
printcp(model)
plotcp(mode)
#testing the model
pred <- predict(model, data = as.data.frame(testset))
#calculating accuracy
accuracy = 1 - mean(pred == testset[ , testset[,11]])
##experiments end here
#fit =  rpart(data[,11] ~ data[,1:10], data=as.data.frame(data))
#rpart.plot(fit)
#class.pred <- table(predict(fit, type="class"), data[,11]) #confusion matrix
#cerror1 = 1-sum(diag(class.pred))/sum(class.pred) #classification error
#cerror1
library('rpart.plot')
install.packages('rpart.plot')
library('rpart.plot')
rpart.plot(model)
dataset
dim(dataset)
accuracy = 1 - mean(pred == testset[ , testset[,11]])
library('rpart')
library('rpart.plot')
dataset = read.table("../Data/page-blocks.data", sep = "")
dataset = as.matrix(dataset)
##experiments start here
bound <- floor((nrow(dataset)/4)*3)         #define % of training and test set
dataset <- dataset[sample(nrow(dataset)), ]           #sample rows
trainset <- dataset[1:bound, ]              #get training set
testset <- dataset[(bound+1):nrow(dataset), ]    #get test set
#creating the model
model <- rpart(trainset[,11] ~ trainset[,1:10], data = as.data.frame(trainset), minbucket=5)
#plotting the tree, along with attributes and split factors
rpart.plot(model)
datatext(model)
#printing and plotting relative error vs cp and size of tree for pruning
printcp(model)
plotcp(mode)
#testing the model
pred <- predict(model, data = as.data.frame(testset))
#calculating accuracy
accuracy = 1 - mean(pred == testset[ , testset[,11]])
##experiments end here
#fit =  rpart(data[,11] ~ data[,1:10], data=as.data.frame(data))
#rpart.plot(fit)
#class.pred <- table(predict(fit, type="class"), data[,11]) #confusion matrix
#cerror1 = 1-sum(diag(class.pred))/sum(class.pred) #classification error
#cerror1
library('rpart')
library('rpart.plot')
dataset = read.table("../Data/page-blocks.data", sep = "")
dataset = as.matrix(dataset)
##experiments start here
bound <- floor((nrow(dataset)/4)*3)         #define % of training and test set
dataset <- dataset[sample(nrow(dataset)), ]           #sample rows
trainset <- dataset[1:bound, ]              #get training set
testset <- dataset[(bound+1):nrow(dataset), ]    #get test set
#creating the model
model <- rpart(trainset[,11] ~ trainset[,1:10], data = as.data.frame(trainset), minbucket=5)
#plotting the tree, along with attributes and split factors
rpart.plot(model)
datatext(model)
#printing and plotting relative error vs cp and size of tree for pruning
printcp(model)
plotcp(mode)
#testing the model
pred <- predict(model, data = as.data.frame(testset))
#calculating accuracy
accuracy = 1 - mean(pred == testset[ , testset[,11]])
accuracy
##experiments end here
#fit =  rpart(data[,11] ~ data[,1:10], data=as.data.frame(data))
#rpart.plot(fit)
#class.pred <- table(predict(fit, type="class"), data[,11]) #confusion matrix
#cerror1 = 1-sum(diag(class.pred))/sum(class.pred) #classification error
#cerror1
library('rpart')
library('rpart.plot')
dataset = read.table("../Data/page-blocks.data", sep = "")
dataset = as.matrix(dataset)
##experiments start here
bound <- floor((nrow(dataset)/4)*3)         #define % of training and test set
dataset <- dataset[sample(nrow(dataset)), ]           #sample rows
trainset <- dataset[1:bound, ]              #get training set
testset <- dataset[(bound+1):nrow(dataset), ]    #get test set
#creating the model
model <- rpart(trainset[,11] ~ trainset[,1:10], data = as.data.frame(trainset), minbucket=5)
#plotting the tree, along with attributes and split factors
rpart.plot(model)
datatext(model)
#printing and plotting relative error vs cp and size of tree for pruning
printcp(model)
plotcp(mode)
#testing the model
pred <- predict(model, data = as.data.frame(testset))
#calculating accuracy
accuracy = 1 - mean(pred == testset[ , testset[,11]])
accuracy
##experiments end here
#fit =  rpart(data[,11] ~ data[,1:10], data=as.data.frame(data))
#rpart.plot(fit)
#class.pred <- table(predict(fit, type="class"), data[,11]) #confusion matrix
#cerror1 = 1-sum(diag(class.pred))/sum(class.pred) #classification error
#cerror1
library('rpart')
library('rpart.plot')
dataset = read.table("../Data/page-blocks.data", sep = "")
dataset = as.matrix(dataset)
#dividing the testing and training dataset
bound <- floor((nrow(dataset)/2))         #define % of training and test set
dataset <- dataset[sample(nrow(dataset)), ]           #sample rows
trainset <- dataset[1:bound, ]              #get training set
testset <- dataset[(bound+1):nrow(dataset), ]    #get test set
#creating the model
model <- rpart(trainset[,11] ~ trainset[,1:10], data = as.data.frame(trainset), minbucket=5)
#plotting the tree, along with attributes and split factors
rpart.plot(model)
datatext(model)
#printing and plotting relative error vs cp and size of tree for pruning
printcp(model)
plotcp(mode)
#testing the model
pred <- predict(model, data = as.data.frame(testset))
#calculating accuracy
accuracy = 1 - mean(pred == testset[ , testset[,11]])
accuracy
prp(model, main="assorted arguments",
nn=TRUE, # display the node numbers
fallen.leaves=FALSE, # put the leaves on the bottom of the page
shadow.col="gray", # shadows under the leaves
branch.lty=3, # draw branches using dotted lines
branch=.5, # change angle of branch lines
faclen=0, # faclen=0 to print full factor names
trace=1, # print the automatically calculated cex
split.cex=1.2, # make the split text larger than the node text
split.prefix="is ", # put "is " before split text
split.suffix="?", # put "?" after split text
col=cols, border.col=cols, # green if survived
split.box.col="lightgray", # lightgray split boxes (default is white)
split.border.col="darkgray", # darkgray border on split boxes
split.round=.5) # round the split box corners a tad
prp(model, main="assorted arguments",
nn=TRUE, # display the node numbers
fallen.leaves=FALSE, # put the leaves on the bottom of the page
shadow.col="gray", # shadows under the leaves
branch.lty=3, # draw branches using dotted lines
branch=.5, # change angle of branch lines
faclen=0, # faclen=0 to print full factor names
trace=1, # print the automatically calculated cex
split.cex=1.2, # make the split text larger than the node text
split.prefix="is ", # put "is " before split text
split.suffix="?", # put "?" after split text
split.box.col="lightgray", # lightgray split boxes (default is white)
split.border.col="darkgray", # darkgray border on split boxes
split.round=.5) # round the split box corners a tad
prp(model, main="Decision tree showing splits",
nn=TRUE, # display the node numbers
fallen.leaves=FALSE, # put the leaves on the bottom of the page
shadow.col="gray", # shadows under the leaves
branch.lty=3, # draw branches using dotted lines
branch=.5, # change angle of branch lines
faclen=0, # faclen=0 to print full factor names
trace=1, # print the automatically calculated cex
split.cex=1.2, # make the split text larger than the node text
split.prefix="is ", # put "is " before split text
split.suffix="?", # put "?" after split text
split.box.col="lightgray", # lightgray split boxes (default is white)
split.border.col="darkgray", # darkgray border on split boxes
split.round=.5) # round the split box corners a tad
datatext(model)
text(model)
prp(model, main="Decision tree showing splits",
nn=TRUE, # display the node numbers
fallen.leaves=FALSE, # put the leaves on the bottom of the page
shadow.col="gray", # shadows under the leaves
branch.lty=3, # draw branches using dotted lines
branch=.5, # change angle of branch lines
faclen=0, # faclen=0 to print full factor names
trace=1, # print the automatically calculated cex
split.cex=1.2, # make the split text larger than the node text
split.prefix="is ", # put "is " before split text
split.suffix="?", # put "?" after split text
split.box.col="lightgray", # lightgray split boxes (default is white)
split.border.col="darkgray", # darkgray border on split boxes
split.round=.5) # round the split box corners a tad
text(model)
rpart.plot(model)
prp(model, main="Decision tree showing splits",
nn=TRUE, # display the node numbers
fallen.leaves=FALSE, # put the leaves on the bottom of the page
shadow.col="gray", # shadows under the leaves
branch.lty=3, # draw branches using dotted lines
branch=.5, # change angle of branch lines
faclen=0, # faclen=0 to print full factor names
trace=1, # print the automatically calculated cex
split.cex=1.2, # make the split text larger than the node text
split.prefix="is ", # put "is " before split text
split.suffix="?", # put "?" after split text
split.box.col="lightgray", # lightgray split boxes (default is white)
split.border.col="darkgray", # darkgray border on split boxes
split.round=.5) # round the split box corners a tad
rpart.plot(model)
text(model)
rpart.plot(model, extra=104, box.palette="GnBu", branch.lty=3, shadow.col="gray", nn=TRUE)
rpart.plot(model, box.palette="GnBu", branch.lty=3, shadow.col="gray", nn=TRUE)
rpart.plot(model)
rpart.plot(model, box.palette="GnBu", branch.lty=3, shadow.col="gray", nn=TRUE)
text(model)
rpart.plot(model, box.palette="GnBu", branch.lty=3, shadow.col="gray", nn=TRUE)
text(model)
library('rpart')
library('rpart.plot')
dataset = read.table("../Data/page-blocks.data", sep = "")
dataset = as.matrix(dataset)
#dividing the testing and training dataset into half-and-half
bound <- floor((nrow(dataset)/2))
dataset <- dataset[sample(nrow(dataset)), ]           #sample rows
trainset <- dataset[1:bound, ]              #get training set
testset <- dataset[(bound+1):nrow(dataset), ]    #get test set
#creating the model
model <- rpart(trainset[,11] ~ trainset[,1:10], data = as.data.frame(trainset), minbucket=5)
#plotting the tree, along with attributes and split factors
rpart.plot(model, box.palette="GnBu", branch.lty=3, shadow.col="gray", nn=TRUE)
#printing and plotting relative error vs cp and size of tree for pruning
printcp(model)
plotcp(mode)
#testing the model
pred <- predict(model, data = as.data.frame(testset))
#calculating accuracy
accuracy = 1 - mean(pred == testset[ , testset[,11]])
print(paste("Accuracy of decision tree on this test set is ", accuracy))
library('rpart')
library('rpart.plot')
dataset = read.table("../Data/page-blocks.data", sep = "")
dataset = as.matrix(dataset)
#dividing the testing and training dataset into half-and-half
bound <- floor((nrow(dataset)/2))
dataset <- dataset[sample(nrow(dataset)), ]           #sample rows
trainset <- dataset[1:bound, ]              #get training set
testset <- dataset[(bound+1):nrow(dataset), ]    #get test set
#creating the model
model <- rpart(trainset[,11] ~ trainset[,1:10], data = as.data.frame(trainset), minbucket=5)
#plotting the tree, along with attributes and split factors
rpart.plot(model, box.palette="GnBu", branch.lty=3, shadow.col="gray", nn=TRUE, fallen.leaves = FALSE)
#printing and plotting relative error vs cp and size of tree for pruning
printcp(model)
plotcp(mode)
#testing the model
pred <- predict(model, data = as.data.frame(testset))
#calculating accuracy
accuracy = 1 - mean(pred == testset[ , testset[,11]])
print(paste("Accuracy of decision tree on this test set is ", accuracy))
library('rpart')
library('rpart.plot')
dataset = read.table("../Data/page-blocks.data", sep = "")
dataset = as.matrix(dataset)
#dividing the testing and training dataset into half-and-half
bound <- floor((nrow(dataset)/2))
dataset <- dataset[sample(nrow(dataset)), ]           #sample rows
trainset <- dataset[1:bound, ]              #get training set
testset <- dataset[(bound+1):nrow(dataset), ]    #get test set
#creating the model
model <- rpart(trainset[,11] ~ trainset[,1:10], data = as.data.frame(trainset), minbucket=5)
#plotting the tree, along with attributes and split factors
rpart.plot(model, box.palette="GnBu", branch.lty=3, shadow.col="gray", nn=TRUE, fallen.leaves = TRUE)
#printing and plotting relative error vs cp and size of tree for pruning
printcp(model)
plotcp(mode)
#testing the model
pred <- predict(model, data = as.data.frame(testset))
#calculating accuracy
accuracy = 1 - mean(pred == testset[ , testset[,11]])
print(paste("Accuracy of decision tree on this test set is ", accuracy))
